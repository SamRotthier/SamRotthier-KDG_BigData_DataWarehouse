{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config stuff",
   "id": "21015048f40aff18"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-25T12:03:46.914912Z",
     "start_time": "2024-12-25T12:03:46.791175Z"
    }
   },
   "source": [
    "from xmlrpc.client import DateTime\n",
    "\n",
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "cc.setupEnvironment()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start local cluster",
   "id": "4baf5e0d3af12170"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:03:55.555570Z",
     "start_time": "2024-12-25T12:03:48.161292Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"FACT_RIDES\")\n",
    "spark.getActiveSession()"
   ],
   "id": "1682df97d406f91b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x17d8e4741d0>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://127.0.0.1:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FACT_RIDES</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create facts table: rides",
   "id": "b9b9b6d7e21766e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read from sources",
   "id": "1380829350c8bdbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read from VeloDB database",
   "id": "8275f9036f4aefa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:04:01.605761Z",
     "start_time": "2024-12-25T12:03:58.464218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACT\n",
    "\n",
    "cc.set_connectionProfile(\"VeloDB\")\n",
    "\n",
    "# Read rides table from source VeloDB database\n",
    "# Only rows after 2019-01-01 will be read because older rows are corrupt\n",
    "rides_source_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\", \"(select rideid, starttime, endtime, subscriptionid, startlockid, endlockid, round(haversine_km(startpoint[0] :: numeric, startpoint[1]:: numeric, endpoint[0]:: numeric, endpoint[1]:: numeric),3) as distance_km from rides where starttime > '2019-01-01') as subq\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"rideid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 4140000) \\\n",
    "    .load()\n",
    "\n",
    "# Next read operation performs a join between the locks and stations source tables. The goal is to retrieve a zipcode for each lockid. This zipcode is used to link the weather dimension with the facts table \n",
    "locks_with_zip = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\", \"(select l.lockid, s.zipcode from locks l \\\n",
    "    left outer join stations s on l.stationid = s.stationid) as subq\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"lockid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 8000) \\\n",
    "    .load()\n",
    "\n",
    "locks_with_zip.show(10)"
   ],
   "id": "f78ee4179ebf769b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|lockid|zipcode|\n",
      "+------+-------+\n",
      "|     1|   2000|\n",
      "|     2|   2000|\n",
      "|     3|   2000|\n",
      "|     4|   2000|\n",
      "|     5|   2000|\n",
      "|     6|   2000|\n",
      "|     7|   2000|\n",
      "|     8|   2000|\n",
      "|     9|   2000|\n",
      "|    10|   2000|\n",
      "+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:04:24.647314Z",
     "start_time": "2024-12-25T12:04:05.486313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "rides_source_df.show(10)\n",
    "#rides_source_df.cache() # De cache loste de memory problemen niet op"
   ],
   "id": "136a30e36181e14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+--------------+-----------+---------+--------------------+\n",
      "|rideid|          starttime|            endtime|subscriptionid|startlockid|endlockid|         distance_km|\n",
      "+------+-------------------+-------------------+--------------+-----------+---------+--------------------+\n",
      "|    15|2019-09-22 08:46:43|2019-09-22 09:01:36|         13296|       4849|     3188|3.443000000000000000|\n",
      "|    16|2019-09-22 08:19:51|2019-09-22 08:21:55|         45924|       NULL|     NULL|0.487000000000000000|\n",
      "|    17|2019-09-22 08:27:38|2019-09-22 08:30:25|         25722|       2046|     1951|0.699000000000000000|\n",
      "|    18|2019-09-22 08:41:48|2019-09-22 08:46:52|         31000|       1821|     2186|1.406000000000000000|\n",
      "|    19|2019-09-22 08:50:08|2019-09-22 09:09:02|         59732|       6382|     2700|4.884000000000000000|\n",
      "|    20|2019-09-22 08:29:42|2019-09-22 08:31:40|          NULL|       NULL|     NULL|0.509000000000000000|\n",
      "|    21|2019-09-22 08:05:17|2019-09-22 08:14:44|         31055|       1388|     3401|1.506000000000000000|\n",
      "|    22|2019-09-22 08:39:11|2019-09-22 08:44:46|         65164|       2572|       13|1.402000000000000000|\n",
      "|    23|2019-09-22 08:23:27|2019-09-22 08:30:02|         71164|         50|     2067|1.744000000000000000|\n",
      "|    24|2019-09-22 08:20:54|2019-09-22 08:22:47|         68426|       NULL|     NULL|0.654000000000000000|\n",
      "+------+-------------------+-------------------+--------------+-----------+---------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[rideid: int, starttime: timestamp, endtime: timestamp, subscriptionid: int, startlockid: int, endlockid: int, distance_km: decimal(38,18)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read from deltatables",
   "id": "918fa361f6468f22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:04:29.974937Z",
     "start_time": "2024-12-25T12:04:27.911374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACT\n",
    "# Dimension date\n",
    "dim_date = spark.read.format(\"delta\").load(\"spark-warehouse/dimdate\")\n",
    "\n",
    "# Dimension weather\n",
    "dim_weather = spark.read.format(\"delta\").load(\"spark-warehouse/dimweather\")\n",
    "\n",
    "# Dimension customer\n",
    "dim_customer = spark.read.format(\"delta\").load(\"spark-warehouse/dimuser\")\n",
    "\n",
    "# Dimension lock\n",
    "dim_lock = spark.read.format(\"delta\").load(\"spark-warehouse/dimlock\")\n",
    "\n"
   ],
   "id": "8ab2574fb49e10fa",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read from weather data source",
   "id": "fe3de972c98c971d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:04:32.331461Z",
     "start_time": "2024-12-25T12:04:31.472712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACT\n",
    "weather_responses = spark.read.format(\"json\").option(\"multiLine\",True).load(\"weather\")\n",
    "weather_responses.show(10)"
   ],
   "id": "2f2acde511b34f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+--------------+----------+-------+--------------------+-----+------+--------------------+--------+----------+--------------------+-----------------+-------+\n",
      "|    base|clouds|cod|         coord|        dt|     id|                main| name|  rain|                 sys|timezone|visibility|             weather|             wind|zipCode|\n",
      "+--------+------+---+--------------+----------+-------+--------------------+-----+------+--------------------+--------+----------+--------------------+-----------------+-------+\n",
      "|stations| {100}|200|{44.34, 10.99}|1583593967|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2000|\n",
      "|stations| {100}|200|{44.34, 10.99}|1583132121|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2018|\n",
      "|stations| {100}|200|{44.34, 10.99}|1583134645|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2020|\n",
      "|stations| {100}|200|{44.34, 10.99}|1585973548|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2030|\n",
      "|stations| {100}|200|{44.34, 10.99}|1569163778|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2050|\n",
      "|stations| {100}|200|{44.34, 10.99}|1569968446|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2060|\n",
      "|stations| {100}|200|{44.34, 10.99}|1575264035|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2100|\n",
      "|stations| {100}|200|{44.34, 10.99}|1619936979|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2140|\n",
      "|stations| {100}|200|{44.34, 10.99}|1656838375|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2170|\n",
      "|stations| {100}|200|{44.34, 10.99}|1659778730|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2600|\n",
      "+--------+------+---+--------------+----------+-------+--------------------+-----+------+--------------------+--------+----------+--------------------+-----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create tempviews",
   "id": "22d0cb85fdb74c02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:04:34.454671Z",
     "start_time": "2024-12-25T12:04:34.364207Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rides source table\n",
    "rides_source_df.createOrReplaceTempView(\"ridesSource\")\n",
    "\n",
    "# Table with lockid's and zipcodes\n",
    "locks_with_zip.createOrReplaceTempView(\"locksZip\")\n",
    "\n",
    "# Dimension date\n",
    "dim_date.createOrReplaceTempView(\"dimDate\")\n",
    "\n",
    "# Dimension weather\n",
    "dim_weather.createOrReplaceTempView(\"dimWeather\")\n",
    "\n",
    "# Weather responses\n",
    "weather_responses.createOrReplaceTempView(\"weatherResponses\")\n",
    "\n",
    "# Dimension customer\n",
    "dim_customer.createOrReplaceTempView(\"dimCustomer\")\n",
    "\n",
    "# Dimension lock\n",
    "dim_lock.createOrReplaceTempView(\"dimLock\")"
   ],
   "id": "cee69800b538c0d7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Add zipcodes to rides table",
   "id": "8b21081cb158aaad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Join ridesSource with locksZip on lockid so we can add the zipcode to the rides table",
   "id": "e10f71814786bcc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T12:05:07.405212Z",
     "start_time": "2024-12-25T12:04:36.489025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRANSFORM\n",
    "rides_with_zipcodes = spark.sql(\"select src.rideid, src.starttime, src.endtime, src.subscriptionid, src.startlockid, \\\n",
    "                                 src.endlockid, src.distance_km, lz.zipcode as startlockZipcode\\\n",
    "                                 from ridesSource as src \\\n",
    "                                 left outer join locksZip as lz on src.startlockid = lz.lockid\")\n",
    "rides_with_zipcodes.show(10)"
   ],
   "id": "d210b507dace370f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\samro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\samro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling z:py4j.reflection.TypeUtil.isInstanceOf",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mPy4JJavaError\u001B[0m                             Traceback (most recent call last)",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    178\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\protocol.py:326\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m answer[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m REFERENCE_TYPE:\n\u001B[1;32m--> 326\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JJavaError(\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    328\u001B[0m         \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name), value)\n\u001B[0;32m    329\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31m<class 'str'>\u001B[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(10061, 'No connection could be made because the target machine actively refused it', None, 10061, None))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[1;31mPy4JError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[8], line 6\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# TRANSFORM\u001B[39;00m\n\u001B[0;32m      2\u001B[0m rides_with_zipcodes \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mselect src.rideid, src.starttime, src.endtime, src.subscriptionid, src.startlockid, \u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;124m                                 src.endlockid, src.distance_km, lz.zipcode as startlockZipcode\u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;124m                                 from ridesSource as src \u001B[39m\u001B[38;5;130;01m\\\u001B[39;00m\n\u001B[0;32m      5\u001B[0m \u001B[38;5;124m                                 left outer join locksZip as lz on src.startlockid = lz.lockid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m----> 6\u001B[0m \u001B[43mrides_with_zipcodes\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshow\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m10\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001B[0m, in \u001B[0;36mDataFrame.show\u001B[1;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[0;32m    887\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mshow\u001B[39m(\u001B[38;5;28mself\u001B[39m, n: \u001B[38;5;28mint\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m20\u001B[39m, truncate: Union[\u001B[38;5;28mbool\u001B[39m, \u001B[38;5;28mint\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m, vertical: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    888\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001B[39;00m\n\u001B[0;32m    889\u001B[0m \n\u001B[0;32m    890\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.3.0\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    945\u001B[0m \u001B[38;5;124;03m    name | Bob\u001B[39;00m\n\u001B[0;32m    946\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 947\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_show_string\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001B[0m, in \u001B[0;36mDataFrame._show_string\u001B[1;34m(self, n, truncate, vertical)\u001B[0m\n\u001B[0;32m    959\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m PySparkTypeError(\n\u001B[0;32m    960\u001B[0m         error_class\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNOT_BOOL\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    961\u001B[0m         message_parameters\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvertical\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marg_type\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28mtype\u001B[39m(vertical)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m},\n\u001B[0;32m    962\u001B[0m     )\n\u001B[0;32m    964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(truncate, \u001B[38;5;28mbool\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m truncate:\n\u001B[1;32m--> 965\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshowString\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvertical\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    966\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    967\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:181\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    179\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m f(\u001B[38;5;241m*\u001B[39ma, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw)\n\u001B[0;32m    180\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m--> 181\u001B[0m     converted \u001B[38;5;241m=\u001B[39m \u001B[43mconvert_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjava_exception\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    182\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[0;32m    183\u001B[0m         \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[0;32m    184\u001B[0m         \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m    185\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\delta\\exceptions.py:160\u001B[0m, in \u001B[0;36m_patch_convert_exception.<locals>.convert_delta_exception\u001B[1;34m(e)\u001B[0m\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m delta_exception \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    159\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m delta_exception\n\u001B[1;32m--> 160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43moriginal_convert_sql_exception\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:159\u001B[0m, in \u001B[0;36mconvert_exception\u001B[1;34m(e)\u001B[0m\n\u001B[0;32m    156\u001B[0m c: Py4JJavaError \u001B[38;5;241m=\u001B[39m e\u001B[38;5;241m.\u001B[39mgetCause()\n\u001B[0;32m    157\u001B[0m stacktrace: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m jvm\u001B[38;5;241m.\u001B[39morg\u001B[38;5;241m.\u001B[39mapache\u001B[38;5;241m.\u001B[39mspark\u001B[38;5;241m.\u001B[39mutil\u001B[38;5;241m.\u001B[39mUtils\u001B[38;5;241m.\u001B[39mexceptionString(e)\n\u001B[0;32m    158\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m c \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m (\n\u001B[1;32m--> 159\u001B[0m     \u001B[43mis_instance_of\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgw\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43morg.apache.spark.api.python.PythonException\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;66;03m# To make sure this only catches Python UDFs.\u001B[39;00m\n\u001B[0;32m    161\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28many\u001B[39m(\n\u001B[0;32m    162\u001B[0m         \u001B[38;5;28mmap\u001B[39m(\n\u001B[0;32m    163\u001B[0m             \u001B[38;5;28;01mlambda\u001B[39;00m v: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124morg.apache.spark.sql.execution.python\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m v\u001B[38;5;241m.\u001B[39mtoString(), c\u001B[38;5;241m.\u001B[39mgetStackTrace()\n\u001B[0;32m    164\u001B[0m         )\n\u001B[0;32m    165\u001B[0m     )\n\u001B[0;32m    166\u001B[0m ):\n\u001B[0;32m    167\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    168\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m  An exception was thrown from the Python worker. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    169\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPlease see the stack trace below.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m c\u001B[38;5;241m.\u001B[39mgetMessage()\n\u001B[0;32m    170\u001B[0m     )\n\u001B[0;32m    171\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m PythonException(msg, stacktrace)\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:464\u001B[0m, in \u001B[0;36mis_instance_of\u001B[1;34m(gateway, java_object, java_class)\u001B[0m\n\u001B[0;32m    460\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    461\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    462\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mjava_class must be a string, a JavaClass, or a JavaObject\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 464\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mgateway\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjvm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpy4j\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreflection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTypeUtil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43misInstanceOf\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    465\u001B[0m \u001B[43m    \u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjava_object\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[0;32m   1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[1;32m-> 1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1323\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[0;32m   1326\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[1;34m(*a, **kw)\u001B[0m\n\u001B[0;32m    177\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdeco\u001B[39m(\u001B[38;5;241m*\u001B[39ma: Any, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkw: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Any:\n\u001B[0;32m    178\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 179\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkw\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    180\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JJavaError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    181\u001B[0m         converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
      "File \u001B[1;32mC:\\BigDataVenv\\.venv\\Lib\\site-packages\\py4j\\protocol.py:334\u001B[0m, in \u001B[0;36mget_return_value\u001B[1;34m(answer, gateway_client, target_id, name)\u001B[0m\n\u001B[0;32m    330\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    331\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m. Trace:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{3}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    332\u001B[0m                 \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name, value))\n\u001B[0;32m    333\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 334\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m Py4JError(\n\u001B[0;32m    335\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAn error occurred while calling \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;132;01m{2}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39m\n\u001B[0;32m    336\u001B[0m             \u001B[38;5;28mformat\u001B[39m(target_id, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m, name))\n\u001B[0;32m    337\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    338\u001B[0m     \u001B[38;5;28mtype\u001B[39m \u001B[38;5;241m=\u001B[39m answer[\u001B[38;5;241m1\u001B[39m]\n",
      "\u001B[1;31mPy4JError\u001B[0m: An error occurred while calling z:py4j.reflection.TypeUtil.isInstanceOf"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T11:58:26.626223Z",
     "start_time": "2024-12-25T11:58:26.609683Z"
    }
   },
   "cell_type": "code",
   "source": "rides_with_zipcodes.createOrReplaceTempView(\"ridesWithZip\")",
   "id": "e586ec69316a116e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transform weather responses table",
   "id": "bb87cc70b13038d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Add weather_ID based on the weather type (condition_id)\n",
    "\n",
    "Condition id:\n",
    "- < 800: All codes with a number smaller than 800 means rain in some form. (=onaangenaam code 2)\n",
    "- = 800: This code means clear sky and sunshine (=aangenaam code 1 if temperature is higher than 15 degrees Celsius)\n",
    "- \\> 800: All other weather conditions (Neutraal code 3)"
   ],
   "id": "c71761d400efc5fc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T11:58:29.426234Z",
     "start_time": "2024-12-25T11:58:29.068763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TRANSFORM\n",
    "short_weather_responses = spark.sql(\"select zipCode as zip_code, dt as timestamp, weather.id[0] as condition_id, main.temp as temperature, \\\n",
    "                                    case \\\n",
    "                                        when condition_id < 800 then 2 \\\n",
    "                                        when condition_id = 800 and main.temp > (273 + 15) then 1 \\\n",
    "                                        when condition_id = 800 and main.temp < (273 + 15) then 3 \\\n",
    "                                        when condition_id > 800 then 3 \\\n",
    "                                    else 4 \\\n",
    "                                    end as weather_ID \\\n",
    "                                    from weatherResponses\")\n",
    "short_weather_responses.show(10)"
   ],
   "id": "3c18954ca1662ff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------+-----------+----------+\n",
      "|zip_code| timestamp|condition_id|temperature|weather_ID|\n",
      "+--------+----------+------------+-----------+----------+\n",
      "|    2000|1583593967|         804|     298.48|         3|\n",
      "|    2018|1583132121|         804|     298.48|         3|\n",
      "|    2020|1583134645|         804|     298.48|         3|\n",
      "|    2030|1585973548|         804|     298.48|         3|\n",
      "|    2050|1569163778|         804|     298.48|         3|\n",
      "|    2060|1569968446|         804|     298.48|         3|\n",
      "|    2100|1575264035|         804|     298.48|         3|\n",
      "|    2140|1619936979|         804|     298.48|         3|\n",
      "|    2170|1656838375|         804|     298.48|         3|\n",
      "|    2600|1659778730|         804|     298.48|         3|\n",
      "+--------+----------+------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T11:58:32.008206Z",
     "start_time": "2024-12-25T11:58:31.996455Z"
    }
   },
   "cell_type": "code",
   "source": "short_weather_responses.createOrReplaceTempView(\"shortWeatherResponses\")",
   "id": "bd06a92167783835",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build facts table",
   "id": "c375e6bee2760aa2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-25T11:58:59.551498Z",
     "start_time": "2024-12-25T11:58:33.788131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TRANSFORM\n",
    "rides_fact_df = spark.sql(\"select src.rideid as ride_ID, dd.date_SK, \\\n",
    "                          coalesce(dw.weather_SK, 3) as weather_SK, \\\n",
    "                          1 as count_MV, \\\n",
    "                          (unix_timestamp(endtime) - unix_timestamp(starttime)) as rideDuration_MV, \\\n",
    "                          md5(concat(src.rideid, dd.date_SK, coalesce(dw.weather_SK, 3), 1, rideDuration_MV)) as md5, \\\n",
    "                          dls.lockid as start_lockid, dle.lockid as end_lockid, \\\n",
    "                          md5(concat(dc.country_code, dc.city, dc.street, dc.number)) as md5_customer,\\\n",
    "                          src.distance_km\\\n",
    "                          from ridesWithZip as src \\\n",
    "                          left outer join dimDate as dd \\\n",
    "                          on cast(src.starttime as DATE) = cast(dd.CalendarDate as DATE) \\\n",
    "                          left outer join shortWeatherResponses wr \\\n",
    "                          on src.startlockZipcode = wr.zip_code \\\n",
    "                          and date_format(src.starttime, 'yyyy-MM-dd-HH') = date_format(from_unixtime(wr.timestamp),'yyyy-MM-dd-HH') \\\n",
    "                          left outer join dimWeather dw on dw.weather_id = wr.weather_ID \\\n",
    "                          left outer join dimLock dls on src.startlockid = dls.lockid \\\n",
    "                          left outer join dimLock dle on src.endlockid = dle.lockid\\\n",
    "                          left outer join dimCustomer dc on src.subscriptionid = dc.subscriptionid\\\n",
    "                          where src.subscriptionid is not null and dc.userid is not null\")\n",
    "\n",
    "rides_fact_df.show(10)"
   ],
   "id": "629d1cdcbba63806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+--------+---------------+--------------------+------------+----------+--------------------+--------------------+\n",
      "|ride_ID|date_SK|weather_SK|count_MV|rideDuration_MV|                 md5|start_lockid|end_lockid|        md5_customer|         distance_km|\n",
      "+-------+-------+----------+--------+---------------+--------------------+------------+----------+--------------------+--------------------+\n",
      "|     26|     21|         3|       1|            641|0495f90b50d12567e...|        2039|      3038|90da6c113a7195acf...|2.545000000000000000|\n",
      "|     27|     21|         3|       1|           1176|7d493b85be00b2a6d...|        5619|      2717|d16d79cc9ef0f3083...|5.576000000000000000|\n",
      "|     34|     21|         3|       1|            677|9e6c18e335cb603d9...|         982|      2015|4e5196b6f2e8f6e48...|2.349000000000000000|\n",
      "|     36|     21|         3|       1|           1066|3994164be2545f135...|        6421|      1208|077b1183566327ab9...|4.154000000000000000|\n",
      "|     43|     21|         3|       1|            391|8c6a7f5b38fdecf27...|        2507|      2365|3087112aec85b5d3f...|1.553000000000000000|\n",
      "|     50|     21|         3|       1|            314|f4d745d86eefd588c...|        2962|      2973|343beb0626fbd6666...|1.328000000000000000|\n",
      "|     77|     21|         3|       1|            488|071567b47f2d1d9cc...|        3165|      2782|af02fd2772928819c...|1.783000000000000000|\n",
      "|     86|     21|         0|       1|            647|0dd0246ba96641b47...|         750|      5746|49e7d0de0847b5ebc...|3.183000000000000000|\n",
      "|     93|     21|         3|       1|           1064|001a454457862e0a9...|        7185|      3112|23200c18c8d3d209b...|6.125000000000000000|\n",
      "|    103|     21|         3|       1|            195|07005a36525fbfc1d...|        2806|      3229|a9e2281a527473fc7...|0.737000000000000000|\n",
      "+-------+-------+----------+--------+---------------+--------------------+------------+----------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Create temview from facts table",
   "id": "2a5d8d8b2b2878a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T10:41:00.190688Z",
     "start_time": "2024-11-26T10:41:00.171849Z"
    }
   },
   "cell_type": "code",
   "source": "rides_fact_df.createOrReplaceTempView(\"factRides_new\")",
   "id": "35c980efea70ba7c",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Write facts table to delta table: Initial load",
   "id": "3982c905d6e38c4b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T10:41:46.940478Z",
     "start_time": "2024-11-26T10:41:34.856056Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LOAD\n",
    "rides_fact_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"factRides\")"
   ],
   "id": "bfc28907c0791b44",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Incremental load",
   "id": "4c7accdf334aa1ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-26T10:42:16.753169Z",
     "start_time": "2024-11-26T10:41:51.881236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#LOAD\n",
    "dt_factRides = DeltaTable.forPath(spark,\".\\spark-warehouse\\\\factrides\")\n",
    "dt_factRides.toDF().createOrReplaceTempView(\"factRides_current\")\n",
    "\n",
    "result = spark.sql(\"merge into factRides_current as target \\\n",
    "                   using factRides_new as source on target.ride_ID = source.ride_ID \\\n",
    "                   when matched and target.md5 <> source.md5 then update set * \\\n",
    "                   when not matched then insert *\")\n",
    "\n",
    "result.show()"
   ],
   "id": "a9f0e0a4e63a43bb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------------+----------------+-----------------+\n",
      "|num_affected_rows|num_updated_rows|num_deleted_rows|num_inserted_rows|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "|                0|               0|               0|                0|\n",
      "+-----------------+----------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-24T15:40:50.933739Z",
     "start_time": "2024-12-24T15:40:50.439677Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "dfa054e8b32a32a2",
   "outputs": [],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
