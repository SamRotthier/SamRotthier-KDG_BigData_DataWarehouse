{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Config stuff",
   "id": "21015048f40aff18"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-12T16:18:39.458766Z",
     "start_time": "2024-11-12T16:18:39.313977Z"
    }
   },
   "source": [
    "import ConnectionConfig as cc\n",
    "from delta import DeltaTable\n",
    "cc.setupEnvironment()"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Start local cluster",
   "id": "4baf5e0d3af12170"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:19:06.922314Z",
     "start_time": "2024-11-12T16:18:44.186540Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spark = cc.startLocalCluster(\"FACT_RIDES\")\n",
    "spark.getActiveSession()"
   ],
   "id": "1682df97d406f91b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x236d024b110>"
      ],
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-5LDFMLTG:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FACT_RIDES</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create facts table: rides",
   "id": "b9b9b6d7e21766e7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Read from sources",
   "id": "1380829350c8bdbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read from VeloDB database",
   "id": "8275f9036f4aefa4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Filteren en joinen in de brontabel of in de spark omgeving? Momenteel gekozen om de rides table en de locks en stations table apart in te lezen en ze later te joinen in de spark omgeving. Zorgt voor minder DB operaties.",
   "id": "610e24c0894aa1a2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:25:07.029887Z",
     "start_time": "2024-11-12T16:25:06.222027Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACT\n",
    "\n",
    "cc.set_connectionProfile(\"VeloDB\")\n",
    "\n",
    "# Read rides table from source VeloDB database\n",
    "rides_source_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\", \"(select rideid, starttime, endtime, subscriptionid, startlockid, endlockid from rides) as subq\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"rideid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 4140000) \\\n",
    "    .load()\n",
    "\n",
    "# Next read operation performs a join between the locks and stations source tables. The goal is to retrieve a zipcode for each lockid. This zipcode is used to link the weather dimension with the facts table \n",
    "locks_with_zip = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", cc.create_jdbc()) \\\n",
    "    .option(\"driver\" , cc.get_Property(\"driver\")) \\\n",
    "    .option(\"dbtable\", \"(select l.lockid, s.zipcode from locks l \\\n",
    "    left outer join stations s on l.stationid = s.stationid) as subq\") \\\n",
    "    .option(\"user\", cc.get_Property(\"username\")) \\\n",
    "    .option(\"password\", cc.get_Property(\"password\")) \\\n",
    "    .option(\"partitionColumn\", \"lockid\") \\\n",
    "    .option(\"numPartitions\", 4) \\\n",
    "    .option(\"lowerBound\", 0) \\\n",
    "    .option(\"upperBound\", 8000) \\\n",
    "    .load()\n",
    "\n",
    "locks_with_zip.show(10)"
   ],
   "id": "f78ee4179ebf769b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+\n",
      "|lockid|zipcode|\n",
      "+------+-------+\n",
      "|     1|   2000|\n",
      "|     2|   2000|\n",
      "|     3|   2000|\n",
      "|     4|   2000|\n",
      "|     5|   2000|\n",
      "|     6|   2000|\n",
      "|     7|   2000|\n",
      "|     8|   2000|\n",
      "|     9|   2000|\n",
      "|    10|   2000|\n",
      "+------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:25:14.289944Z",
     "start_time": "2024-11-12T16:25:12.741952Z"
    }
   },
   "cell_type": "code",
   "source": "rides_source_df.show(10)",
   "id": "136a30e36181e14",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+--------------+-----------+---------+\n",
      "|rideid|          starttime|            endtime|subscriptionid|startlockid|endlockid|\n",
      "+------+-------------------+-------------------+--------------+-----------+---------+\n",
      "|     1|2015-09-22 00:00:00|2012-09-22 00:00:00|         13296|       4849|     3188|\n",
      "|     2|2015-09-22 00:00:00|2012-09-22 00:00:00|         45924|       NULL|     NULL|\n",
      "|     3|2015-09-22 00:00:00|2012-09-22 00:00:00|         25722|       2046|     1951|\n",
      "|     4|2015-09-22 00:00:00|2012-09-22 00:00:00|         31000|       1821|     2186|\n",
      "|     5|2015-09-22 00:00:00|2012-09-22 00:00:00|         59732|       6382|     2700|\n",
      "|     6|2015-09-22 00:00:00|2012-09-22 00:00:00|          NULL|       NULL|     NULL|\n",
      "|     7|2015-09-22 00:00:00|2012-09-22 00:00:00|         31055|       1388|     3401|\n",
      "|     8|2015-09-22 00:00:00|2012-09-22 00:00:00|         65164|       2572|       13|\n",
      "|     9|2015-09-22 00:00:00|2012-09-22 00:00:00|         71164|         50|     2067|\n",
      "|    10|2015-09-22 00:00:00|2012-09-22 00:00:00|         68426|       NULL|     NULL|\n",
      "+------+-------------------+-------------------+--------------+-----------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read from deltatables",
   "id": "918fa361f6468f22"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:19:32.698975Z",
     "start_time": "2024-11-12T16:19:30.191938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACT\n",
    "# Dimension date\n",
    "dim_date = spark.read.format(\"delta\").load(\"spark-warehouse/dimdate\")\n",
    "\n",
    "# Dimension weather\n",
    "dim_weather = spark.read.format(\"delta\").load(\"spark-warehouse/dimweather\")"
   ],
   "id": "8ab2574fb49e10fa",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Read from weather data source",
   "id": "fe3de972c98c971d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:19:39.144659Z",
     "start_time": "2024-11-12T16:19:37.472695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#EXTRACT\n",
    "weather_responses = spark.read.format(\"json\").option(\"multiLine\",True).load(\"weather\")\n",
    "weather_responses.show(10)"
   ],
   "id": "2f2acde511b34f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---+--------------+----------+-------+--------------------+-----+------+--------------------+--------+----------+--------------------+-----------------+-------+\n",
      "|    base|clouds|cod|         coord|        dt|     id|                main| name|  rain|                 sys|timezone|visibility|             weather|             wind|zipCode|\n",
      "+--------+------+---+--------------+----------+-------+--------------------+-----+------+--------------------+--------+----------+--------------------+-----------------+-------+\n",
      "|stations| {100}|200|{44.34, 10.99}|1730638800|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2000|\n",
      "|stations| {100}|200|{44.34, 10.99}|1730912400|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2018|\n",
      "|stations| {100}|200|{44.34, 10.99}|1731160800|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2020|\n",
      "|stations| {100}|200|{44.34, 10.99}|1730638800|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2030|\n",
      "|stations| {100}|200|{44.34, 10.99}|1730912400|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2050|\n",
      "|stations| {100}|200|{44.34, 10.99}|1731160800|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2060|\n",
      "|stations| {100}|200|{44.34, 10.99}|1730638800|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2100|\n",
      "|stations| {100}|200|{44.34, 10.99}|1730912400|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2140|\n",
      "|stations| {100}|200|{44.34, 10.99}|1731160800|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2170|\n",
      "|stations| {100}|200|{44.34, 10.99}|1730638800|3163858|{298.74, 933, 64,...|Zocca|{3.16}|{IT, 2075663, 166...|    7200|     10000|[{overcast clouds...|{349, 1.18, 0.62}|   2600|\n",
      "+--------+------+---+--------------+----------+-------+--------------------+-----+------+--------------------+--------+----------+--------------------+-----------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create tempviews",
   "id": "22d0cb85fdb74c02"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:25:29.091561Z",
     "start_time": "2024-11-12T16:25:29.004067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rides source table\n",
    "rides_source_df.createOrReplaceTempView(\"ridesSource\")\n",
    "\n",
    "# Table with lockid's and zipcodes\n",
    "locks_with_zip.createOrReplaceTempView(\"locksZip\")\n",
    "\n",
    "# Dimension date\n",
    "dim_date.createOrReplaceTempView(\"dimDate\")\n",
    "\n",
    "# Dimension weather\n",
    "dim_weather.createOrReplaceTempView(\"dimWeather\")\n",
    "\n",
    "# Weather responses\n",
    "weather_responses.createOrReplaceTempView(\"weatherResponses\")"
   ],
   "id": "cee69800b538c0d7",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Add zipcodes to rides table",
   "id": "8b21081cb158aaad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Join ridesSource with lockStationsSource on lockid",
   "id": "e10f71814786bcc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:32:37.683588Z",
     "start_time": "2024-11-12T16:32:32.436547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TRANSFORM\n",
    "rides_with_zipcodes = spark.sql(\"select src.rideid, src.starttime, src.endtime, src.subscriptionid, src.startlockid, \\\n",
    "                                 src.endlockid, lz.zipcode as startlockZipcode \\\n",
    "                                 from ridesSource as src \\\n",
    "                                 left outer join locksZip as lz on src.startlockid = lz.lockid\")\n",
    "rides_with_zipcodes.show(10)"
   ],
   "id": "d210b507dace370f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+-------------------+--------------+-----------+---------+----------------+\n",
      "|rideid|          starttime|            endtime|subscriptionid|startlockid|endlockid|startlockZipcode|\n",
      "+------+-------------------+-------------------+--------------+-----------+---------+----------------+\n",
      "|     4|2015-09-22 00:00:00|2012-09-22 00:00:00|         31000|       1821|     2186|            2018|\n",
      "|     9|2015-09-22 00:00:00|2012-09-22 00:00:00|         71164|         50|     2067|            2000|\n",
      "|     1|2015-09-22 00:00:00|2012-09-22 00:00:00|         13296|       4849|     3188|            2140|\n",
      "|     5|2015-09-22 00:00:00|2012-09-22 00:00:00|         59732|       6382|     2700|            2660|\n",
      "|    11|2015-09-22 00:00:00|2012-09-22 00:00:00|           999|        985|     2148|            2018|\n",
      "|     2|2015-09-22 00:00:00|2012-09-22 00:00:00|         45924|       NULL|     NULL|            NULL|\n",
      "|     3|2015-09-22 00:00:00|2012-09-22 00:00:00|         25722|       2046|     1951|            2000|\n",
      "|     6|2015-09-22 00:00:00|2012-09-22 00:00:00|          NULL|       NULL|     NULL|            NULL|\n",
      "|     7|2015-09-22 00:00:00|2012-09-22 00:00:00|         31055|       1388|     3401|            2018|\n",
      "|     8|2015-09-22 00:00:00|2012-09-22 00:00:00|         65164|       2572|       13|            2000|\n",
      "+------+-------------------+-------------------+--------------+-----------+---------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Transform weather responses table",
   "id": "bb87cc70b13038d2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:07:22.691162Z",
     "start_time": "2024-11-12T16:07:22.312553Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TRANSFORM\n",
    "short_weather_responses = spark.sql(\"select zipCode as zip_code, dt as timestamp, weather.id[0] as condition_id, main.temp as temperature from weatherResponses\")\n",
    "short_weather_responses.show(10)"
   ],
   "id": "3c18954ca1662ff7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------+-----------+\n",
      "|zip_code| timestamp|condition_id|temperature|\n",
      "+--------+----------+------------+-----------+\n",
      "|    2000|1730638800|         804|     298.48|\n",
      "|    2018|1730912400|         804|     298.48|\n",
      "|    2020|1731160800|         804|     298.48|\n",
      "|    2030|1730638800|         804|     298.48|\n",
      "|    2050|1730912400|         804|     298.48|\n",
      "|    2060|1731160800|         804|     298.48|\n",
      "|    2100|1730638800|         804|     298.48|\n",
      "|    2140|1730912400|         804|     298.48|\n",
      "|    2170|1731160800|         804|     298.48|\n",
      "|    2600|1730638800|         804|     298.48|\n",
      "+--------+----------+------------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:08:53.647342Z",
     "start_time": "2024-11-12T16:08:53.630318Z"
    }
   },
   "cell_type": "code",
   "source": "short_weather_responses.createOrReplaceTempView(\"shortWeatherResponses\")",
   "id": "bd06a92167783835",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Build facts table",
   "id": "c375e6bee2760aa2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Koppeling met date dimensie:\n",
    "- Moet endtime ook een SK hebben naar de date dimensie? Want zorgt voor extra kolom --> Niet goed in feitentabel!\n",
    "- Eventueel van de kolom date_SK de koppeling naar starttime en endttime samenvoegen?"
   ],
   "id": "a304ef55717e6a3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:07:37.252573Z",
     "start_time": "2024-11-12T16:07:29.828591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#TRANSFORM\n",
    "rides_fact_df = spark.sql(\"select src.rideid as ride_ID, dd.date_SK \\\n",
    "                          from ridesSource as src \\\n",
    "                          left outer join dimDate as dd on cast(src.starttime as DATE) = cast(dd.CalendarDate as DATE) \\\n",
    "                          where src.subscriptionid is not null \")\n",
    "\n",
    "rides_fact_df.show(50)"
   ],
   "id": "629d1cdcbba63806",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|ride_ID|date_SK|\n",
      "+-------+-------+\n",
      "|      1|   1096|\n",
      "|      2|   1096|\n",
      "|      3|   1096|\n",
      "|      4|   1096|\n",
      "|      5|   1096|\n",
      "|      7|   1096|\n",
      "|      8|   1096|\n",
      "|      9|   1096|\n",
      "|     10|   1096|\n",
      "|     11|   1096|\n",
      "|     12|   1096|\n",
      "|     13|   1096|\n",
      "|     14|   1096|\n",
      "|     15|   2557|\n",
      "|     16|   2557|\n",
      "|     17|   2557|\n",
      "|     18|   2557|\n",
      "|     19|   2557|\n",
      "|     21|   2557|\n",
      "|     22|   2557|\n",
      "|     23|   2557|\n",
      "|     24|   2557|\n",
      "|     25|   2557|\n",
      "|     26|   2557|\n",
      "|     27|   2557|\n",
      "|     28|   2557|\n",
      "|     29|   2557|\n",
      "|     31|   2557|\n",
      "|     32|   2557|\n",
      "|     33|   2557|\n",
      "|     34|   2557|\n",
      "|     35|   2557|\n",
      "|     36|   2557|\n",
      "|     37|   2557|\n",
      "|     38|   2557|\n",
      "|     39|   2557|\n",
      "|     40|   2557|\n",
      "|     41|   2557|\n",
      "|     42|   2557|\n",
      "|     43|   2557|\n",
      "|     44|   2557|\n",
      "|     45|   2557|\n",
      "|     46|   2557|\n",
      "|     47|   2557|\n",
      "|     48|   2557|\n",
      "|     49|   2557|\n",
      "|     50|   2557|\n",
      "|     51|   2557|\n",
      "|     53|   2557|\n",
      "|     54|   2557|\n",
      "+-------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T15:52:01.644532Z",
     "start_time": "2024-11-12T15:51:59.471675Z"
    }
   },
   "cell_type": "code",
   "source": "spark.stop()",
   "id": "4440b06d5b5e52a6",
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[WinError 10061] Kan geen verbinding maken omdat de doelcomputer de verbinding actief heeft geweigerd",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mConnectionRefusedError\u001B[0m                    Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[9], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\datawarehouse\\Lib\\site-packages\\pyspark\\sql\\session.py:1796\u001B[0m, in \u001B[0;36mSparkSession.stop\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1782\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1783\u001B[0m \u001B[38;5;124;03mStop the underlying :class:`SparkContext`.\u001B[39;00m\n\u001B[0;32m   1784\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1792\u001B[0m \u001B[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001B[39;00m\n\u001B[0;32m   1793\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   1794\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpyspark\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msql\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontext\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SQLContext\n\u001B[1;32m-> 1796\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1797\u001B[0m \u001B[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001B[39;00m\n\u001B[0;32m   1798\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jvm \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\datawarehouse\\Lib\\site-packages\\pyspark\\context.py:654\u001B[0m, in \u001B[0;36mSparkContext.stop\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    652\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_jsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    653\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 654\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    655\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m Py4JError:\n\u001B[0;32m    656\u001B[0m         \u001B[38;5;66;03m# Case: SPARK-18523\u001B[39;00m\n\u001B[0;32m    657\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    658\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    659\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m It is possible that the process has crashed,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    660\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m been killed or may also be in a zombie state.\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    661\u001B[0m             \u001B[38;5;167;01mRuntimeWarning\u001B[39;00m,\n\u001B[0;32m    662\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\datawarehouse\\Lib\\site-packages\\py4j\\java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m   1314\u001B[0m args_command, temp_args \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_args(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m   1316\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1317\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1318\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[0;32m   1319\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m-> 1321\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msend_command\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcommand\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1322\u001B[0m return_value \u001B[38;5;241m=\u001B[39m get_return_value(\n\u001B[0;32m   1323\u001B[0m     answer, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_id, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname)\n\u001B[0;32m   1325\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
      "File \u001B[1;32m~\\PycharmProjects\\datawarehouse\\Lib\\site-packages\\py4j\\java_gateway.py:1036\u001B[0m, in \u001B[0;36mGatewayClient.send_command\u001B[1;34m(self, command, retry, binary)\u001B[0m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msend_command\u001B[39m(\u001B[38;5;28mself\u001B[39m, command, retry\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, binary\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m   1016\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001B[39;00m\n\u001B[0;32m   1017\u001B[0m \u001B[38;5;124;03m       called directly by Py4J users. It is usually called by\u001B[39;00m\n\u001B[0;32m   1018\u001B[0m \u001B[38;5;124;03m       :class:`JavaMember` instances.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1034\u001B[0m \u001B[38;5;124;03m     if `binary` is `True`.\u001B[39;00m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 1036\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1037\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1038\u001B[0m         response \u001B[38;5;241m=\u001B[39m connection\u001B[38;5;241m.\u001B[39msend_command(command)\n",
      "File \u001B[1;32m~\\PycharmProjects\\datawarehouse\\Lib\\site-packages\\py4j\\clientserver.py:284\u001B[0m, in \u001B[0;36mJavaClient._get_connection\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    281\u001B[0m     \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[0;32m    283\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m connection \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m connection\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m--> 284\u001B[0m     connection \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_new_connection\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    285\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[1;32m~\\PycharmProjects\\datawarehouse\\Lib\\site-packages\\py4j\\clientserver.py:291\u001B[0m, in \u001B[0;36mJavaClient._create_new_connection\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    287\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_create_new_connection\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    288\u001B[0m     connection \u001B[38;5;241m=\u001B[39m ClientServerConnection(\n\u001B[0;32m    289\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_parameters, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpython_parameters,\n\u001B[0;32m    290\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_property, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m--> 291\u001B[0m     \u001B[43mconnection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconnect_to_java_server\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_thread_connection(connection)\n\u001B[0;32m    293\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m connection\n",
      "File \u001B[1;32m~\\PycharmProjects\\datawarehouse\\Lib\\site-packages\\py4j\\clientserver.py:438\u001B[0m, in \u001B[0;36mClientServerConnection.connect_to_java_server\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    435\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context:\n\u001B[0;32m    436\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mssl_context\u001B[38;5;241m.\u001B[39mwrap_socket(\n\u001B[0;32m    437\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket, server_hostname\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address)\n\u001B[1;32m--> 438\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mconnect((\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_address, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjava_port))\n\u001B[0;32m    439\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msocket\u001B[38;5;241m.\u001B[39mmakefile(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    440\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_connected \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[1;31mConnectionRefusedError\u001B[0m: [WinError 10061] Kan geen verbinding maken omdat de doelcomputer de verbinding actief heeft geweigerd"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
